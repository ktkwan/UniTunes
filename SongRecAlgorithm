# approach: https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50
# K closest: https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761

import csv
import numpy as np
import random
import operator
from sklearn.cluster import KMeans

#attributes that you want to store
_genres = []
_duration = []
_popularity = []
_jaccardScores = []
_k = 10 # number of clusters
_newData = []
_centroids= []
_trackNames = []


# main function
def main():
    _trackNames, _genres,_duration, _popularity = readInData()
    avgGenre= computeAverageGenre(_genres)
    _jaccardScores = computeGenreSimilarity(avgGenre, _genres)
    # get rid of the headlines
    _duration = _duration[1:]
    _popularity = _popularity[1:]
    _trackNames = _trackNames[1:]
    # combine three attributes in to one array numbSongs x 3
    for i in range(len(_genres)-1): # accomodate for the headers
        row = []
        j = _jaccardScores[i]
        p = _popularity[i]
        d = _duration[i]
        row.append(j)
        row.append(p)
        row.append(d)
        _newData.append(np.array(row))
    _centroids = init_centroids()
    # find the closest centroids based on the data
    #closest = closest_centroids(_newData, _centroids)
    #print(closest)

    _centroids,closest_centroids = run(_newData,_trackNames, _centroids)
    print(_centroids, closest_centroids)


# function that reads in and parses the data from a csv
def readInData():
    with open('data/songs_with_genre.csv') as songs_with_genre:
        csv_reader = csv.reader(songs_with_genre, delimiter=',')
        line_count = 0
        for row in csv_reader:
            if (row[5] is not None and row[6] is not None and row[7] is not None):
                _trackNames.append(row[0])
                _genres.append(row[5])
                _duration.append(row[6])
                _popularity.append(row[7])
    return _trackNames, _genres,_duration,_popularity

#function that processes the similarity between two genres using jacard similarity, repeats don't matter.
def jaccardSimilarity(str1, str2):
    a = set(str1.split())
    b = set(str2.split())
    c = a.intersection(b)
    return float(len(c))/(len(a) + len(b) - len(c))

def computeAverageGenre(genres):
    """
    creates a new aray with normalized scores for each genre
    """
    # first creates a basis vector out of the most frequently visited words
    dict = {}
    length_genre = 0
    for genre in genres:
        words = genre.split(" ")
        for word in words:
            if word not in dict:
                dict[word] = 1
            else:
                dict[word] += 1
        length_genre+= len(words)
    avg_length = float(length_genre)/float(len(genres)) # found that the average is 1.875, so we will use a vector of size 2.
    sorted_d = sorted(dict.items(), key=operator.itemgetter(1),reverse=True)
    title1 = sorted_d[0][0]
    title2 = sorted_d[1][0]
    normalizedTitle = title1 + " " + title2
    return normalizedTitle

def computeGenreSimilarity(normalizedTitle, genres):
    """ finds the similarity between each title and the normalized title
    input: normalizedTitle - the average title
    genres - all the genres in the data
    return: an array with the similarity scores for each genre entry in the data
    """
    jaccardScores = []
    for genre in genres:
        jaccard = jaccardSimilarity(normalizedTitle, genre)
        jaccardScores.append(jaccard)
    return jaccardScores

def init_centroids():
	"""
	Selects k random rows from inputs and returns them as the chosen centroids
	:return: a Numpy array of k cluster centroids, one per row
	"""
	length_data = len(_genres) - 1
	rand = np.arange(length_data-1)
	np.random.shuffle(rand)
	_centroids = []
	for r in range(0, _k):
		_centroids = np.append(_centroids, _newData[r])
	_centroids = np.reshape(_centroids,(_k,len(_newData[0])))
	_centroids = np.array(_centroids,dtype = 'float64')
	return _centroids

def euclidean_dist(x, y):
	"""
	Computes the Euclidean distance between two points, x and y

	:param x: the first data point, a Python numpy array
	:param y: the second data point, a Python numpy array
	:return: the Euclidean distance between x and y
	"""
	x = x.astype(np.float)
	y = y.astype(np.float)
	dist = np.linalg.norm(x - y)
	return dist


def closest_centroids(data, centroids):
	"""
	Computes the closest centroid for each data point in X, returning
	an array of centroid indices

	:return: an array of centroid indices
	"""
	centroidIndices = []
	for d in data:
		dist = np.sum((centroids-d.astype(np.float))**2, axis = 1)
		index = np.argmin(dist)
		centroidIndices.append(index)
	return centroidIndices

def compute_centroids(centroid_indices, centroids, data):
	"""
	Computes the centroids for each cluster, or the average of all data points
	in the cluster

	:param centroid_indices: a Numpy array of centroid indices, one for each datapoint in X
	"""
	centroidSet = set()
	count = 0
	for index in centroid_indices:
		centroidSet.add(index)
	for centroid in centroidSet:
		results = []
		for i in range(0,len(data)):
			if(centroid_indices[i] == centroid):
				results.append(data[i])
		results = np.array(results,dtype = 'float64')
		centroids[count] = np.average(results)
		count+=1

def run(data, names, centroids):
	"""
	Run the k-means algorithm on dataset X with K clusters.
	Make sure to call closest_centroids and compute_centroids!
	Check for convergence here, or whether you hit the max number of iterations.

	return: centroids, closest_centroids
	"""
	closest = []
	for i in range(0, 1000):
		closest = closest_centroids(data, centroids)
		compute_centroids(closest, centroids, data)
	for i in range(0, len(closest)):
        # print out everything classified to the 0th centroid
		if closest[i] == 0:
		    print(names[i])
	return(centroids,closest)

#executable
if __name__ == "__main__":
    main()
